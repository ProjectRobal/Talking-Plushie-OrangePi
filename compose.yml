version: '3.9'

networks:
  internal:
    name: "infranet"
  external:
    name: "internet"
    external: true

services:
  chat:
    build: 
      context: ./chatbot
      dockerfile: Dockerfile
    # platform: linux/amd64
    env_file:
      - ./chatbot/envs
    volumes:
    #- ./chatbot/models/q4_0-orca-mini-3b.gguf:/app/models/model.gguf
    #  - ./chatbot/models/Lite-Mistral-150M-v2-Instruct-Q4_0.gguf:/app/models/model.gguf
    # - ./chatbot/models/ggml-mambahermes-3b-q4_k.gguf:/app/models/model.gguf
    - ./chatbot/models/tinyllama-1.1b-chat-v1.0.Q4_0.gguf:/app/models/model.gguf
    #  - ./chatbot/models/mamba-130m.Q4_0.gguf:/app/models/model.gguf
    - ./chatbot/prompt.txt:/app/prompt.txt
    networks:
      - internal
    expose:
      - 8080:8080
    ports:
      - 8080:8080

  rag:
    image: qdrant/qdrant
    # platform: linux/amd64
    volumes:
      - ./rag:/qdrant/storage
    networks:
      - internal
    expose:
      - 6333:6333
    ports:
      - 6333:6333

  stt:
    build: 
      context: ./stt
      dockerfile: Dockerfile
    # platform: linux/amd64
    env_file:
      - ./stt/envs
    volumes:
      - ./stt/models/ggml-tiny-q5_1.bin:/app/models/model.bin
    networks:
      - internal
    expose:
      - 8080:8080
    ports:
      - 8081:8080
  
  client:
   build: 
     context: ./client
     dockerfile: Dockerfile
   restart: always
   depends_on:
    rag:
        condition: service_started
   tmpfs: 
    - /run/shared:size=100000
   volumes:
     - ./client/tts/en_GB-alan-low.onnx:/app/tts/model.onnx
     - ./client/tts/en_GB-alan-low.onnx.json:/app/tts/model.onnx.json
     - /run/shared:/app/files
     #- ./test:/app/files
     - ./client/src/main.py:/app/main.py
     - ./client/embed/:/app/embed/
   env_file:
     - ./client/envs
   devices:
     - /dev/snd:/dev/snd
   networks:
    #  - external
     - internal
